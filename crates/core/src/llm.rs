use std::time::Duration;

/// A struct representing a Large Language Model (LLM)
#[derive(Debug, Default, Clone)]
pub struct LLM {
    /// The name or identifier of the model to use
    /// Examples: "gpt-4", "gpt-3.5-turbo", etc.
    pub model: String,
    /// The timeout duration for the request
    /// If the request does not complete within this time, it will return a timeout error
    pub timeout: Option<Duration>,
    /// Controls the randomness of the model's text generation
    /// Higher values make the output more random, while lower values make it more deterministic
    /// Typically ranges from 0.0 to 1.0
    pub temperature: Option<f64>,
    /// A system prompt (preamble) to guide the model's behavior
    /// For example, a fixed prompt can be used to direct the model to generate specific types of text
    pub preamble: Option<String>,
    /// The maximum number of tokens for the completion text
    /// Limits the length of the text generated by the model
    pub max_completion_tokens: Option<usize>,
    /// The maximum number of tokens for the entire request
    /// Includes both the input and the generated text
    pub max_tokens: Option<usize>,
    /// The base URL for the API
    /// Can be used to specify a custom API endpoint
    pub base_url: Option<String>,
    /// The version of the API
    /// Some APIs may require a version number to ensure compatibility
    pub api_version: Option<String>,
    /// The API key used for authentication
    /// Typically required to access protected API services
    pub api_key: Option<String>,
    /// The context window size, representing the maximum number of tokens the model can handle
    /// Used to limit the total length of input and output
    pub context_window_size: usize,
}
