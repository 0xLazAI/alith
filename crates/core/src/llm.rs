pub mod client;

use serde::{Deserialize, Serialize};
use std::time::Duration;

// OpenAI models

pub const GPT_4: &str = "gpt-4";
pub const GPT_4_32K: &str = "gpt-4-32k";
pub const GPT_4_TURBO: &str = "gpt-4-turbo";
pub const GPT_3_5_TURBO: &str = "gpt-3.5-turbo";
pub const GPT_4O_MINI: &str = "gpt-4o-mini";

// Anthropic models

pub const CLAUDE_3_OPUS: &str = "claude-3-opus";
pub const CLAUDE_3_SONNET: &str = "claude-3-sonnet";
pub const CLAUDE_3_HAIKU: &str = "claude-3-haiku";
pub const CLAUDE_3_5_SONNET: &str = "claude-3-5-sonnet";

// Remote Llama models

pub const LLAMA_3_1_SONAR_SMALL_ONLINE: &str = "llama-3.1-sonar-small-128k-online";
pub const LLAMA_3_1_SONAR_LARGE_ONLINE: &str = "llama-3.1-sonar-large-128k-online";
pub const LLAMA_3_1_SONAR_HUGE_ONLINE: &str = "llama-3.1-sonar-huge-128k-online";
pub const LLAMA_3_1_SONAR_SMALL_CHAT: &str = "llama-3.1-sonar-small-128k-chat";
pub const LLAMA_3_1_SONAR_LARGE_CHAT: &str = "llama-3.1-sonar-large-128k-chat";
pub const LLAMA_3_1_8B_INSTRUCT: &str = "llama-3.1-8b-instruct";
pub const LLAMA_3_1_70B_INSTRUCT: &str = "llama-3.1-70b-instruct";

/// A struct representing a Large Language Model (LLM)
#[derive(Debug, Default, Clone, Serialize, Deserialize)]
pub struct LLM {
    /// The name or identifier of the model to use
    /// Examples: "gpt-4", "gpt-3.5-turbo", etc.
    pub model: String,
    /// The timeout duration for the request
    /// If the request does not complete within this time, it will return a timeout error
    pub timeout: Option<Duration>,
    /// Controls the randomness of the model's text generation
    /// Higher values make the output more random, while lower values make it more deterministic
    /// Typically ranges from 0.0 to 1.0
    pub temperature: Option<f64>,
    /// A system prompt (preamble) to guide the model's behavior
    /// For example, a fixed prompt can be used to direct the model to generate specific types of text
    pub preamble: Option<String>,
    /// The maximum number of tokens for the completion text
    /// Limits the length of the text generated by the model
    pub max_completion_tokens: Option<usize>,
    /// The maximum number of tokens for the entire request
    /// Includes both the input and the generated text
    pub max_tokens: Option<usize>,
    /// The base URL for the API
    /// Can be used to specify a custom API endpoint
    pub base_url: Option<String>,
    /// The version of the API
    /// Some APIs may require a version number to ensure compatibility
    pub api_version: Option<String>,
    /// The API key used for authentication
    /// Typically required to access protected API services
    pub api_key: Option<String>,
    /// The context window size, representing the maximum number of tokens the model can handle
    /// Used to limit the total length of input and output
    pub context_window_size: usize,
}

impl LLM {
    pub fn new(model: &str) -> Self {
        Self {
            model: model.to_string(),
            timeout: Some(Duration::from_secs(60)),
            temperature: Some(0.7),
            preamble: None,
            max_completion_tokens: Some(512),
            max_tokens: Some(2048),
            base_url: None,
            api_version: None,
            api_key: None,
            context_window_size: 4096,
        }
    }

    pub fn with_temperature(mut self, temperature: f64) -> Self {
        if (0.0..=1.0).contains(&temperature) {
            self.temperature = Some(temperature);
        } else {
            panic!("Temperature must be in the range 0.0 to 1.0.");
        }
        self
    }

    pub fn with_api_key(mut self, api_key: &str) -> Self {
        self.api_key = Some(api_key.to_string());
        self
    }

    pub fn with_context_window_size(mut self, size: usize) -> Self {
        self.context_window_size = size;
        self
    }
}
