//! llama_cpp for local LLM
